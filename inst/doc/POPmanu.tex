%\VignetteEngine{knitr::knitr}
% some terminal commands to copy once and scroll when needed
% cd '/home/xverhoef/data/2019_papers/POP/POP_package/POP/inst/doc/'
% cd '/media/jay/data/desktop_data/2019_papers/POP/POP_package/POP/inst/doc'
% Rscript -e "library(knitr); knit('POPmanu1.Rnw')"
% Rscript -e "library(knitr); purl('POPmanu.Rnw')"
% pdflatex POPmanu2
% bibtex POPmanu2

\documentclass[11pt, titlepage]{article}
\usepackage{geometry}
\geometry{verbose,letterpaper,tmargin=2.54cm,bmargin=2.54cm,lmargin=2.54cm,rmargin=2.54cm}
\usepackage{graphicx, amsmath, natbib, setspace}
\usepackage{float}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{relsize}
\usepackage{subfigure}
\usepackage{pgf}
%\usepackage{/home/xverhoef/data/shTex/mymacros}
\usepackage{mymacros}
\usepackage{pdflscape}
\usepackage{bbding}
\usepackage{fancyvrb}
\usepackage[shortlabels]{enumitem}
\setlength{\parindent}{3em} 
%\onehalfspacing
\doublespacing
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{etoolbox}
\usepackage{lineno}

\AtBeginEnvironment{tabular}{\singlespacing}
\pdfpagewidth 8.5in
\pdfpageheight 11in
\setlength{\oddsidemargin}{0.0in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0.15in} \setlength{\textheight}{8.5in}
\setlength{\headheight}{0.0in} \setlength{\headsep}{0.0in}
%\renewcommand{\abstractname}{Summary}
%\renewcommand{\theequation}{(\arabic{equation})}
\setcounter{figure}{0}

\makeatletter

%\renewcommand\tagform@[1]{\maketag@@@{\ignorespaces#1\unskip\@@italiccorr}}
\makeatother



\begin{document}

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 																	TITLE
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------

\begin{titlepage}
  \begin{center}
\vspace*{2cm}
{\LARGE Species Density Models from Opportunistic Citizen Science Data} \\
\vspace{1cm}
\textbf{Jay M. Ver Hoef$^1$, Devin Johnson$^1$, Robyn Angliss$^1$, Matt Higham$^2$}  \\
\vspace{1cm}
\hrulefill \\ 
\begin{onehalfspace}
  \begin{flushleft}
$^1$Marine Mammal Laboratory, NOAA-NMFS Alaska Fisheries Science Center\\
\hspace{.5cm} 7600 Sand Point Way NE, Seattle, WA 98115\\
\hspace{.5cm} E-mail: jay.verhoef@noaa.gov \hspace{.5cm} tel: (907) 347-5552\\
$^2$Department of Statistics \\
\hspace{.5cm} St. Lawrence University, Canton, NY 
\end{flushleft}
\hrulefill \\
\end{onehalfspace}
\vspace{.5cm}
Running Headline: SDMs from Opportunistic Citizen Science Data \\
\hrulefill \\
\today
  \end{center}
\end{titlepage}

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 														   	ABSTRACT
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
%\setpagewiselinenumbers
\modulolinenumbers[5]
\linenumbers{}

\section*{ABSTRACT}
\thispagestyle{empty}
\begin{onehalfspace}
\begin{itemize}
    \item With the advent of technology for data-gathering and storage, opportunistic citizen-science data are proliferating. Species distribution models (SDMs) aim to use species occurrence or abundance for ecological insights, prediction, and management. We analyzed a massive opportunistic data set with over 100,000 records of incidental shipboard observations of marine mammals. Our overall goal was to create maps of species density from massive opportunistic data by using spatial regression for count data with an effort offset. We illustrate the method with two marine mammals in the Gulf of Alaska and Bering Sea.  
    \item We counted the total number of animals in 11,424 hexagons based on presence-only data. To decrease bias, we first estimated a spatial density surface for ship-days, which was our proxy variable for effort.  We used spatial considerations to create pseudo-absences, and left some hexagons as missing values. Next, we created SDMs that used modeled effort to create pseudo-absences, and included the effort surface as an offset in a second stage analysis of two example species, northern fur seals and Steller sea lions.
    \item For both effort and species counts, we used spatial negative-binomial regression with random effects that had a multivariate normal distribution with a conditional autoregressive (CAR) covariance matrix, providing 2 million Markov chain Monte Carlo (MCMC) samples (2000 were retained) from the posterior distribution. We used a novel MCMC scheme that maintained sparse precision matrices for observed and missing data when batch sampling from the multivariate normal distribution. We also used a truncated normal distribution to stabilize estimates, and used a look-up table for sampling the autocorrelation parameter.  These innovations allowed us to draw several million samples in just a few hours. 
    \item From the posterior distributions of the SDMs, we computed two functions of interest.  We normalized the SDMs and then applied an overall abundance estimate obtained from the literature to derive spatially explicit abundance estimates, especially within subsetted areas.  We also created ``certain hotspots'' that scaled local abundance by standard deviation and using thresholds.  Hexagons with values above a threshold were deemed as hotspots with enough evidence to be certain about them.
\end{itemize}
\end{onehalfspace}
\hrulefill \\

\noindent {\sc Key Words:} Conditional autoregressive, species distribution models, opportunistic data, marine mammals, density models\\


% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 															INTRODUCTION
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------


\begin{spacing}{1.9}
\begin{flushleft}
\setlength{\parindent}{1cm}
\setcounter{page}{0}


\section{INTRODUCTION}

Natural resource surveys conducted with a structured sampling design are the preferred method for most ecological assessments as they provide results with the highest levels of confidence for the species or system of interest. However, more recently, citizen science efforts are being incorporated into scientific research, including data collection, analyses, and interpretation \citep{miller-rushing_history_2012}. Species distribution models (SDMs) \citep{elith_species_2009, guisan_predicting_2013}, which aim to use species occurrence or abundance for ecological insights and prediction, are as fundamental as the definition of ecology itself \citep{Krebs1972Ecologyexperimentalanalysis}. At the intersection of citizen science and SDMs are opportunistic data sets of species occurrence/abundance \citep{soroye_opportunistic_2018} and analytical methods \citep{elith_novel_2006} that are rapidly growing \citep{renner_point_2015, lukyanenko_citizen_2020}.  Our overall goal is to create maps of species density from massive opportunistic data sets by using spatial regression for count data with an effort offset.

Most citizen science data are obtained without a formal sampling design, and hence, according to \citet{kelling_using_2019}, there is no ``fully statistically defensible way of accounting for the biases inherent in the data collection.'' Statistical bias occurs when the expected value of a statistical technique is different from the true quantity that it is estimating.  A sampling design controls the distribution of observation and measurement effort, which generally allows for unbiased estimates. For example, an unbiased estimator for a population total, $\tau$, introduced by  \citet{horvitz_generalization_1952}, is,
\begin{equation} \label{eq:HorvThom}
  \hat{\tau} = \sum_{i=1}^n y_i/\pi_i,
\end{equation}
where $y_i, i = 1, 2, \ldots, N,$ is a value from $N$ population units, $\pi_i$ is the probability that unit $i$ was included in the sample, $n$ ($< N$) is sample size, and $\tau$ is the sum of $\{y_i\}$ over all $N$ sample units. The $\{\pi_i\}$ are probabilities, but it is interesting to view them as the ``effort'' to include an observation, which is sufficient for unbiased estimation. 

Also consider Poisson regression for rates.  We often have counts that need to be put on an equal basis with other counts.  For example, the number of diseases among a county population, the number of animals per geographic area, or the number of manufacturing errors per unit time.  Let $y_{i}$ be the count, then $y_{i}/\pi_{i}$ is the rate (diseases per person, animals per area, or manufacturing errors per time), where here we use $\pi_{i}$ to be county population, geographic area, and time, respectively.  We assume that $y_{i}$ is the result of a random variable, $Y_{i}$, and a reasonable model is $\textrm{E}[Y_{i}]/\pi_{i} = \bx_{i}\upp\bbeta$, where $\bx_{i}$ is a vector of covariates associated with the $i$th observation and $\bbeta$ is a vector of regression parameters.  For most count distributions (e.g., Poisson, negative binomial, etc.), we model the log of the mean as linear, so $\log(\textrm{E}[Y_{i}]/\pi_{i}) = \bx_{i}\upp\bbeta$, where the distribution provides the error terms.  The value $\pi_{i}$ is fixed, so we move the denominator of the rate to the right of the equal sign,
\begin{equation} \label{eq:PoiRegOff}
  \log(\textrm{E}[Y_{i}]) = \log(\pi_{i}) + \bx_{i}\upp\bbeta,
\end{equation} 
and $\log(\pi_{i})$ is known as the ``offset,'' which can be considered as another covariate with known regression coefficient equal to one.

In both Equations~\eqref{eq:HorvThom} and \eqref{eq:PoiRegOff} we used notation $\pi_{i}$ to make the connection that dividing by a probability, an area, or some proxy to effort, is needed; without $\{\pi_i\}$, we do not know how to properly weight $\{y_i\}$ to construct an unbiased estimator. Citizen science data, where observations are collected opportunistically, often lack information on effort, which is a main source of bias \citep{bird_statistical_2014}.

There are now many review articles for SDMs \citep[e.g.,][]{austin_species_2007,
elith_species_2009, hefley_hierarchical_2016,
robinson_systematic_2017, araujo_standards_2019}. Data types might be counts, presence-absence, or presence-only \citep{hefley_hierarchical_2016}. Models can be primarily focused on estimating relationships to covariates, similar to the basic ecological idea of a niche \citep{soberon_grinnellian_2007,elith_species_2009}, and/or focused on prediction in space \citep{austin_spatial_2002,elith_species_2009}.  There is an uneasy relationship between models formed in the covariate (niche) space and coordinate (geographic) space \citep{randin_are_2006}, and here we will focus purely on geographic space.

One of the most popular SDM methods is MAXENT \citep{phillips_maximum_2006}, based on maximum entropy modelling, which uses covariates in a spatially-explicit grid, and models the presence/absence of species in the grid. This is equivalent to Poisson regression \citep{renner_equivalence_2013}, and there are further connections to spatial point processes, but we will stay in the grid framework. An inhomogeneous spatial point process that is aggregated to plots with nonzero areas leads to Equation~\ref{eq:PoiRegOff} \citep{warton_poisson_2010}, where the offset can allow for unequal areas.  It is also possible to add a spatial random error term \citep{guelat_effects_2018}, which we will feature in our model development below.

When citizen science data consists of presence-only, or only nonzero counts, researchers have often created pseudo-absences, or zeros in the data, in an attempt to model where individuals do not occur \citep{pearce_modelling_2006, conn_extrapolating_2015}. A simple approach is to create zeros at random from all plots other than those with observed values \citep{stockwell_controlling_2002}, but better approaches correct for sampling bias \citep{phillips_sample_2009, conn_confronting_2017} and include case-control methods \citep{fithian_local_2014} and local background sampling \citep{daniel_efficient_2020}, among others. We will propose new ideas based on spatial considerations to create pseudo-absences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Motivating Example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Motivating Example} \label{sec:motex}

A Platforms of Opportunity (POP) data set of marine mammal sightings from predominantly ships, including National Oceanic and Atmospheric Administration (NOAA), U.S. Coast Guard, Navy, fishing, research, and tourist vessels, was collected from 1958 - 2016, containing 109,465 records. Data collection and quality control were described in \citet{HimesBoorEtAl2012Stellersealion}. Each record in the POP data set is a marine mammal sighting event, which was the observation of one or more individuals of a single species, and also included date, latitude and longitude, and estimated number of animals. Sightings have been provided to the database by individuals with training and experience that ranged from professional, experienced biologists with extensive knowledge of species identification to members of the public with little or no training.

Our study area consists of the Bering Sea and Gulf of Alaska, situated between Alaska (USA) and Siberia (Russia) (Figure~\ref{Fig-study_area}A). Additionally, we are especially interested in marine mammal density within the dashed area in the Gulf of Alaska.  This is a Density Extent Area (DEA) where, within a much smaller area called the Temporary Maritime Activities Area (TMAA), the U.S. Navy conducts activity simulations, including acoustic signals that may be harmful to marine mammals. Environmental impacts on marine mammals are required by law. We gridded the study area into $N$ = 11,424 hexagons, with a close-up provided in Figure~\ref{Fig-study_area}B. One goal will be providing marine mammal density surfaces within the whole DEA for further subsetting as needed. 

Within the study area, we will consider northern fur seal (\textit{Callorhinus ursinus}) and Steller sea lion (\textit{Eumetopias jubatus}) counts from the POP data set, as each illustrate different results.  These data, from 1958 - 2000, were analyzed by \citet{HimesBoorEtAl2012Stellersealion} with a non-spatial model, and we borrow their idea of a ship-day.    A ship-day was defined as the presence of one or more marine mammal observations from a single ship on a single day, and ship-days were counted by hexagon (Figure~\ref{Fig-raw_data}A).  We assume that ship-days are proportional to effort.  Other approaches might count the actual number of all animals seen in a hexagon as a proxy for effort, but ship-days alleviates the need to make a single marine mammal density a function of all animal density \citep{HimesBoorEtAl2012Stellersealion}.

The numbers of ship-days (Figure~\ref{Fig-raw_data}A) shows a clear sampling bias, and it is not surprising given shipping routes and other activities by various vessels.  Many records are from NOAA ships, whose mission includes ocean charting and research on fish and marine mammals, explaining the dense effort in certain areas.  The total counts per hexagon for northern fur seals and Steller sea lions are shown in Figures~\ref{Fig-raw_data}B and \ref{Fig-raw_data}C, respectively.  The problem is clear.  High counts of northern fur seals and Steller sea lions might be due to high effort, and if either were not counted in a hexagon, it may be due to lack of animals, or lack of effort.  However, the fact that we have data that we assume to be proportional to effort will allow us to adjust for it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Objective
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Objectives} \label{sec:obj}

Our overall objective is to develop a new approach to species distribution models based exclusively on ideas motivated by spatial autocorrelation for count data from gridded plots.  Our specific objectives are to 1) use spatial count regression to develop models for effort, 2) develop spatial count regression SDMs that include effort, 3) provide novel considerations for creating zeros based on spatial autocorrelation rather than covariates, 4) outline a novel Markov Chain Monte Carlo (MCMC) approach for these data and models, 5) combine a normalized SDM with a separate species abundance estimate to provide spatially explicit abundance estimates, and 6) show several useful outcomes that can be provided by computing on the posterior distribution of the SDMs.

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 															  METHODS
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------

\section{METHODS}

We illustrate the methods using the motivating example in Section~\ref{sec:motex}, and use terminology like ship-days to be concrete, but it is just a surrogate for any variable on effort.  Likewise, we will use ``animals'' as the count variable, but these could be any count variable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Models}

Consider a spatial negative binomial regression, where the latent spatial error is multivariate normal with a covariance matrix specified by a conditional autoregressive (CAR) structure \citep[e.g.,][]{VerHoefEtAl2018Spatialautoregressivemodels36}.  Let $\bc = (C_{1}, C_{2}, \ldots, C_{N})$ be a vector of count random variables for ship-days indexed by $N$ total hexagons in Figure~\ref{Fig-study_area}B.  We assume
\begin{equation} \label{eq:poireg}
  [\bc|\alpha_{0},\br,\nu_{e}] = \textrm{NB}(\exp(\alpha_{0} + \br),\nu_{e})
\end{equation}
where $\alpha_{0}$ is an intercept term, $\br$ is a vector of spatially autocorrelated random effects, and $[a|b]$ indicates a conditional distribution where random variable(s) $a$ depends on $b$, and NB$(\bldeta,\nu)$ is a negative binomial distribution with mean vector $\bldeta$ and variance vector $\bldeta + \bldeta^{2}/\nu$. Let $[\br|\sigma^{2}_{e}, \rho_{e}] = \textrm{MVN}_{CAR}(\bzero,\bSigma_{e})$ be spatial random effects where $\textrm{MVN}_{CAR}$ is a zero-mean multivariate normal distribution with covariance matrix $\bSigma_{e} = \sigma^{2}(\bI - \rho_{e}\bW_{e})\upi\bM_{e}$ structured as a conditional autoregressive (CAR) model \citep{Besag1974SpatialInteractionStatistical192}.  The matrix $\bM$ is diagonal, and $\bW$ is sparse, having non-zero values only for neighboring indexes.  That is, in general (for non-edge hexagons), each row of $\bW$ has 6 non-zero values (interior hexagons in Figure~\ref{Fig-study_area}B have 6 neighboring cells).  For the non-zero values, we used row-standardization \citep{VerHoefEtAl2018Spatialautoregressivemodels36}.  By considering flat priors for $-\infty < \alpha_{0} < \infty$, $0 < \sigma^{2}_{e} < \infty$, $-\infty < \nu_{e} < \infty$, and $0 < \rho_{e} < 1$, we obtain the posterior distribution $[\br, \alpha_{0}, \rho_{e}, \sigma^{2}_{e}|\bc]$.

In practice, we use Markov chain Monte Carlo (MCMC) methods to sample from the posterior distribution $[\br, \alpha_{0}, \rho_{e}, \sigma^{2}_{e}|\bc]$, which also provides a sample from the posterior distribution of $[\be|\bc]$, where $\be = \exp(\alpha_{0} + \br)$ is the modeled effort in ship-days back on the nominal scale.  More details on our MCMC methods are given below, and in the Appendix.

Now, let the count of the species of interest (in our motivating example, that is either northern fur seals or Steller sea lions) be a vector of random variables $\by = (Y_{1}, Y_{2}, \ldots, Y_{N})$ with the same indexes as $\bc$ from the hexagons in Figure~\ref{Fig-study_area}B.  We assume
\begin{equation} \label{eq:poiregoff}
[\by|\beta_{0},\bz,\be,\nu_{y}] = \textrm{NB}(\exp(\beta_{0} + \log(\be) + \bz), \nu_{y}).
\end{equation}
Here, $[\bz|\sigma^{2}_{z}, \rho_{z}] = \textrm{MVN}_{CAR}(\bzero,\bSigma_{z})$ is a vector of spatial random effects, $Z_{i}$, with covariance matrix $\bSigma_{z} = \sigma_{z}^{2}(\bI - \rho_{z}\bW_{z})\upi\bM_{z}$ structured as a CAR model.  In Equation~\eqref{eq:poiregoff} $\log(\be)$ is an offset that adjusts for effort. Note that we have adopted the recommendation of \citet{warton_model-based_2013} by modeling the observer bias first ($\be$), and then condition on it to model the species distribution.

As for Equation~\eqref{eq:poireg}, we use Markov chain Monte Carlo (MCMC) methods to sample from the posterior distribution $[\bz, \beta_{0}, \rho_{z}, \sigma^{2}_{z},\nu_{y}|\bc,\be]$. Note that we use a two-stage analysis, where we first model $\be$, condition on it, and then use MCMC sampling to integrate over it, 
\begin{equation} \label{eq:twostage}
[\bz, \beta_{0}, \rho_{z}, \sigma^{2}_{z},\nu_{z}|\by,\bc] = \int[\bz, \beta_{0}, \rho_{z}, \sigma^{2}_{z},\nu_{z}|\by,\be][\be|\bc]d\be.
\end{equation}
We approximate Equation~\eqref{eq:twostage} by sampling from the posterior distribution of $[\be|\bc]$ while we are sampling from the posterior of $[\bz, \beta_{0}, \rho_{z}, \sigma^{2}_{z},\nu_{z}|\by,\be]$. This idea of composition sampling within MCMC has also been used in, e.g., \citet{BanerjeeEtAl2008Gaussianpredictiveprocess825}, \citet{hooten_agent-based_2010}, and \citet{babcock_lidar_2015}. Intuitively, this passes along all uncertainty in $\be$. Again, we considered flat priors for $-\infty < \beta_{0} < \infty$, $0 < \sigma^{2}_{z} < \infty$, $0 < \nu^{2}_{z} < \infty$,and $0 < \rho_{z} < 1$.

It would also be possible to use a Poisson distribution (or other count distribution) in Equation \eqref{eq:poiregoff} rather than a negative binomial.  Figure~\ref{Fig-raw_data}B shows that counts range from 0 to 5,146 for northern fur seals, and from 0 to 11,747 for Steller sea lions, and are highly overdispersed, even with a spatial random effect.  We tried both distributions, and the inclusion of $\nu$ was far from $\infty$ for both species, , so we present the negative binomial results. At $\nu = \infty$, the first two moments are equivalent for negative binomial and Poisson \citep{VerHoefEtAl2007QuasiPoissonvsnegative2766}, so we may as well choose the simpler Poisson model.  The posterior distribution for $\nu$ was very large when modeling ship-days, so a Poisson distribution could have been used.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Computing on the Posterior Distribution
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computing on the Posterior Distribution}

One of the reasons that we chose MCMC sampling is that we want samples from the full joint distribution of all parameters, which will allow us to compute functions of interest from the whole spatial surface.  Because we have MCMC samples of the whole surface, we also have MCMC samples of any quantity computed on that whole surface.  Hence, we easily obtain standard errors and uncertainty quantification when computing on the posterior distribution of the spatial surface.  This is in contrast to a method such as INLA \citep{rue_approximate_2009}, which is very fast, but only gives the marginal variance of each spatial location.  We give two examples.  The first is to compute the total number of animals in the DEA (described in Section \ref{sec:motex}).

Let $\theta_{i}$ be the true expected number of animals in hexagon $i$.  Then
\[
\theta^{*}_{i} = \frac{\theta_{i}}{\sum_{i=1}^{N}\theta_{i}}
\]
is ``standardized relative abundance'' (SRA). Note that if the total abundance is known, $T = \sum \theta_{i}$, then $T\theta^{*}_{i} = \theta_{i}$ allows the recovery of each hexagon's abundance from a total abundance estimate and SRA. From Equation~\eqref{eq:poiregoff}, let $\mu_{i} =\exp(\beta_{0} + Z_{i})$, which is expected counts per unit effort in the $i$th hexagon. A reasonable assumption is that $\mu_{i}$ is proportional to $\theta_{i}$; $\mu_{i} = \phi\theta_{i}$.  We standardize $\{\mu_i\}$,
\[
\mu^{*}_{i} = \frac{\mu_{i}}{\sum_{i=1}^{N}\mu_{i}}  = \frac{\theta_{i}}{\sum_{i=1}^{N}\theta_{i}} = \theta_{i}^{*},
\]
which allows for recovery of each hexagon's abundance if we have a separate total abundance estimate, $\theta_{i} = T\theta^{*}_{i} = T\mu^{*}_{i}$.  The MCMC sample from Equation~\eqref{eq:twostage} provides a sample from the posterior distribution of $[\bmu|\by]$, where $\bmu = \exp(\beta_{0} + \bz)$, which is the expected number of animals per hexagon for a single ship-day, back on the nominal scale. If we have a posterior distribution for $T$, then sampling from posteriors of both $T$ and $\mu^{*}_{i}$ provides a sample of the posterior for the abundance in each hexagon, and MCMC allows a sample from their joint distribution as well. Turning to the DEA, let $\mathcal{A} = \{1,2,...,n\}$ be the set of indexes for all hexagons, and let $\mathcal{M} \subset \mathcal{A}$ be the set of indexes for the DEA. Then $\tau = \sum_{i \in \mathcal{M}} \theta_{i}$ is the total number of animals in the DEA. Let $\mu^{*}_{i,k}$ be the $k$th MCMC sample for $\mu^{*}_{i}$.  Then the $k$th MCMC sample for the posterior distribution of abundance within the DEA is
\begin{equation} \label{eq:spexlicitabu}
\hat{\tau}_{k} = T\sum_{i \in \mathcal{M}}\mu^{*}_{i,k}, \hspace{.2cm} \textrm{or} \hspace{.2cm}\hat{\tau}_{k}= T_{k}\sum_{i \in \mathcal{M}}\mu^{*}_{i,k}
\end{equation}
depending on whether $T$ is fixed, or $T_{k}$ is the $k$th MCMC sample for $T$. Inferences, such as mean, median, mode, standard deviation, credible intervals, etc., can be obtained from $\{\hat{\tau}_{k}; k = 1,\ldots,K\}$ for K MCMC samples. Thus, although modeling counts with an effort offset does not yield true abundance per hexagon, it can be a key piece of information if a total abundance estimate is available. 

As a second example of computing on the joint posterior distribution, consider the idea of trying to obtain ``certain hotspots'' of animal abundance, which accounts for areas where we are certain abundance is above average, and discounts areas where abundance estimation may be high, but highly uncertain.  To help visualize areas of higher abundance, it is generally desirable to perform some amount of smoothing.  Let $\mathcal{N}_{i} \subset \mathcal{A}$ be the set of indexes in some neighborhood of hexagon $i$, including $i$. Neighborhoods could be those hexagons within a certain radius, or a fixed number of nearest neighbors, etc. Then a smoothed value at location $i$ is
\begin{equation} \label{eq:smoomui}
s_{i} = \frac{\sum_{j \in \mathcal{N}_{i}}\mu_{j}}{|\mathcal{N}_{i}|}
\end{equation}
where $|\mathcal{N}_{i}|$ is the number of neighbors.  Using the $k$th MCMC posterior sample of $\mu_{j,k}$ for the $j$th hexagon, we obtain the $k$th MCMC sample $s_{i,k}$ for $s_{i}$.  Take the mean of the MCMC samples for each $i$; call it $\bar{s}_i$, and let the ordered values, from smallest to largest, be denoted $\bar{s}_{(1)}, \bar{s}_{(2)}, \ldots, \bar{s}_{(n)}$.  Let $q$ be a quantile of interest, say 0.95. Then $\{\bar{s}_{([qn])}, \bar{s}_{([qn]+1)}, \ldots, \bar{s}_{(n)}\}$ are the top 5\% of sites with the highest estimated abundance, where $[a]$ rounds up. Let $\sigma_i$ be the standard deviation among MCMC samples, $s_{i,k}$, for each $i$, and let $\sigma_{(1)}, \sigma_{(2)}, \ldots, \sigma_{(n)}$ be $\{\sigma_i\}$ in the same order as $\bar{s}_{(1)}, \bar{s}_{(2)}, \ldots, \bar{s}_{(n)}$. Then 
\begin{equation} \label{eq:hotspots}
\left\{ \frac{\bar{s}_{([qn])}}{\sigma_{([qn])}}, \frac{\bar{s}_{([qn]+1)}}{\sigma_{([qn]+1)}}, \ldots, \frac{\bar{s}_{(n)}}{\sigma_{(n)}} \right\}
\end{equation}
can be viewed as standard normal values, which will be large when $\bar{s}_i$ is high and $\sigma_{i}$ is low, and, if sufficiently large, we can be certain a site has above average abundance. Cut-off values can be computed by comparing each value in Equation~\eqref{eq:hotspots} to a quantile in the standard normal distribution. A cutoff can be proposed, and only values above the cutoff are claimed to be ``certain hotspots.''  For example, an $\alpha$-level of 0.95 yields the familiar 1.96 as a cutoff value.  However, correcting for 11,424 possible comparisons, and using the conservative Bonferronni adjustment for multiple comparisons, we obtain a cutoff value of 4.59.  We declare any value in Equation~\eqref{eq:hotspots} above 4.59 to be a certain hotspot.  Of course, many other options exist for creating various thresholds of interest.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MCMC Overview
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{MCMC Overview}

We used MCMC methods to obtain samples from the posterior distribution for all parameters and latent random effects.  Here, we give the broad outline of our MCMC sampling scheme, which contained some innovations.  More details are given in the Appendix.  The models in Equations~\eqref{eq:poireg} and \eqref{eq:poiregoff} are substantially the same, so we write the problem generically as sampling from the hierarchical model,
$$
[\by|\beta,\bz_{o},\nu][(\bz_{o}\upp,\bz_{m}\upp)\upp|\bW,\bM,\sigma,\rho][\nu][\sigma][\rho]
$$
where $[\by|\beta,\bz_{o},\nu]$ is a count model with $\textrm{E}[\by] = \exp(\beta + \bz_{o})$ and possibly an extra parameter $\nu$, $[(\bz_{o}\upp,\bz_{m}\upp)\upp|\bW,\bM,\sigma,\rho]$ is a multivariate normal distribution with a CAR model covariance matrix $\bSigma = \sigma^{2}(\bI - \rho\bW)\upi\bM$, and $[\nu][\sigma][\rho]$ are prior distributions. Adding effort, using its posterior distribution, was described for Equation \eqref{eq:twostage}, and because it does not require updating, we ignore it here. Note that the dimension of $\by$ is not the same as $\bz$.  Therefore, it will be necessary to split $\bz = (\bz_{o}\upp,\bz_{m}\upp)\upp$ into hexagons with observed data $\bz_{o}$, and missing data $\bz_{m}$.  Moreover, there is an issue about what are zeros, and what are missing values.  Originally, $[\by|\beta,\bz_{o},\nu]$ has no zeros. For ship-days (effort), there are only hexagons with at least one ship-day, and for animals, there are only hexagons where at least one animal was counted (Figure~\ref{Fig-raw_data}).  So one problem is where to add zeros, which we mentioned in the Introduction.  We discuss our specific approaches while giving examples.  Secondly, even when zeros have been added, there will still be hexagons with missing data.  

Our hierarchical model leads to the following posterior distribution, $[\beta,\bz_{o},\bz_{m},\sigma,\rho,\nu|\by,\bW,\bM]$, and we discuss Metropolis-Hastings sampling from the conditional distribution for each quantity in turn.

\begin{itemize}
  \item $[\beta|\bz_{o},\bz_{m},\sigma,\rho,\nu,\by,\bW,\bM]$ We use a Metropolis step involving the ratio $[\by|\beta^{*},\bz_{o},\nu]/[\by|\beta,\bz_{o},\nu]$, where $\beta^{*}$ is a proposal from a symmetric distribution. 
  \item $[\nu|\beta,\bz_{o},\bz_{m},\sigma,\rho,\by,\bW,\bM]$ We use a Metropolis step involving the ratio $[\by|\beta,\bz_{o},\nu^{*}]/[\by|\beta,\bz_{o},\nu]$, where $\nu^{*}$ is a proposal from a symmetric distribution.
  \item $[\bz_{o}|\beta,\bz_{m},\sigma,\rho,\nu,\by,\bW,\bM]$ We use a Metropolis step involving the ratio
  \begin{equation} \label{eq:MetroZ}
   \frac{[\by|\beta,\bz_{o}^{*},\nu][\bz_{o}^{*}|\bz_{m},\bW,\bM,\sigma,\rho]}{[\by|\beta,\bz_{o},\nu][\bz_{o}|\bz_{m},\bW,\bM,\sigma,\rho]},
  \end{equation}
  where $\bz_{o}^{*}$ is a batch proposal by adding small, independent normal increments to the current values of $\bz_{o}$. It would be more typical to sample each $z_{i}|\bz_{-i},\ldots$ one-at-a-time, where $\bz_{-i}$ contains all the rest of $\{z_{j}; j \ne i\}$, as this derives directly from the conditional definition of the CAR model.  Although no matrix inverses are required, this is still quite slow, looping through all 11,424 hexagons for a single MCMC sample.  The evaluation of $[\by|\beta,\bz_{o},\nu]$ is very fast because all $y_{i}$ are assumed conditionally independent. Note that, in the multivariate normal distribution for all $\bz$, $\bz_{o}$ occurs only in $\exp(-\bz\bSigma\upi\bz/2)$, where $\bSigma\upi = \bM\upi(\bI - \rho\bW)/\sigma^{2}$, and $\bW$ is a sparse matrix and the only inverse required is $\bM\upi$, which is diagonal, making matrix computations fast, with less storage. In the Appendix, we show how that sparse structure can be maintained, even when splitting $\bz = (\bz_{o}, \bz_{m})$. A single batch update is then very fast, and although the independent increments need to be very small for acceptance, thousands can be proposed in the time it takes for a single MCMC loop when sampling one-at-a-time.  Additionally, we assumed a truncated multivariate normal distribution, rather than the usual multivariate normal distribution, and we explain why next.
  \item $[\bz_{m}|\beta,\bz_{o},\sigma,\rho,\nu,\by,\bW,\bM]$ All $\bz_{m}$ are contained only in the multivariate CAR model, so we could use Gibbs sampling one-at-a-time, directly using the definition of a CAR model. However, batch sampling with Metropolis was faster, as described above.  Additionally, we noticed that the MCMC sampler for missing values, especially, was unstable when there were many missing values, lots of zeros, and high overdispersion.  The reason is that the model can always fit the zeros better by making $Z_{i}$, corresponding to an observed $y_{i} = 0$, more and more negative in $\exp(\beta + \bz)$, and adjusting $\beta$ downward so the larger values of $\bz$ still fit the observed $y_{i} > 0$.  This drives the overall variance up for $\bz$.  Missing data are not anchored by observed values, and the larger variance in $\bz$ occasionally leads to extremely large (unrealistic) values after exponentiation, for some $\exp(\beta + Z_{i})$ with missing values.  Similar results were found by \citet{conn_using_2015} and \citet{higham_spatial_2019}. We tried a strong prior on $\sigma^{2}$, but this did not solve the problem, as individual $Z_{i}$ could still get very large when exponentiated.  Truncating the $Z$-values was a simple solution.  It still allowed good fits to the data, and it was easy and fast to make proposals from a truncated multivariate normal distribution.  Thus, ultimately, we did not use a typical CAR multivariate normal distribution, but rather one that was truncated.  Truncation was easy to implement in our framework because it only involved renormalizing the distribution, which canceled in the Metropolis sampling for MCMC (Equation~\ref{eq:MetroZ}) when sampling $\bz_{o}$, and in the ratio $[\bz_{m}^{*}|\bz_{o},\bW,\bM,\sigma,\rho]/[\bz_{m}|\bz_{o},\bW,\bM,\sigma,\rho]$ when sampling $\bz_{m}$.
  \item $[\rho|\beta,\bz,\sigma,\nu,\by,\bW,\bM]$. Sampling for $\bz$ only involves the exponent in the (truncated) multivariate normal distribution, $\exp(-\bz\upp\bSigma\upi\bz)$, and for CAR models, no matrix inverses are necessary.  However, $\rho$ is contained in both $\bSigma\upi$ and the determinant $|\bSigma|$ of a multivariate normal distribution.  Therefore, for each MCMC iteration, we needed to compute $|\bSigma|$, which was very time consuming for an $11,424 \times 11,424$ matrix.  Instead, we pre-computed $|\bSigma|$ for $\textrm{logit}(\rho) = (-40, -39, \ldots, 39, 40)/5$ in $|\sigma^{2}(\bI - \rho\bW)\upi\bM|$.  Special methods exist for determinants of sparse matrices, and because of relations between determinants and inverses, the 81 determinants only took a few minutes, and were stored as a look-up table during MCMC sampling.  Thus, $\rho$ was only sampled on the grid of values with pre-computed determinants, and the Metropolis ratio $[\bz|\bW,\bM,\sigma,\rho^{*}]/[\bz|\bW,\bM,\sigma,\rho]$ could be rapidly evaluated with proposal $\rho^{*}$.  The proposed $\rho^{*}$ was chosen by sampling the 3 nearest values on either side of the current value of $\rho$, all with equal probability, from the set with precomputed determinants.  If the current value of $\rho$ was near either endpoint, so that it did not have 3 values above or below it, we used Hastings sampling due to the asymmetric proposal.
  \item $[\sigma|\beta,\bz,\rho,\nu,\by,\bW,\bM]$  Typically, $\sigma$ could be sampled with an inverse-chi-squared distribution \citep{gelman_bayesian_2013} because it only occurs in $[\bz|\bW,\bM,\rho,\sigma]$. However, due to truncation changing the normalizing constant, we used a Metropolis step involving the ratio $[\bz|\bW,\bM,\rho,\sigma^{*}]/[\bz|\bW,\bM,\rho,\sigma]$, where $\log(\sigma^{*})$ has a symmetric proposal distribution.   

\end{itemize} 

The major innovations in our MCMC scheme were splitting $\bz = (\bz_{o}\upp,\bz_{m}\upp)$ and maintaining sparse matrices without the need to compute matrix inverses when batch sampling for the multivariate normal distribution, using a truncated normal distribution, and using a look-up table for sampling $\rho$.  These innovations allowed us to draw several million samples in just a few hours.  We first used a burn-in time, and tuned the sampler so that Metropolis acceptance rates were between 0.2 and 0.5.  We then used 2 million samples for the final run, retaining only one in every thousand, yielding 2,000 MCMC samples from the full posterior distribution.  We only kept 2,000 MCMC samples because we needed to store all 11,424 $Z$-values for each sample.

We evaluated MCMC convergence using effective sample size \citep[ESS,][]{flegal_markov_2008, gong_practical_2016} and MCMC standard error.  The minimum ESS among all $\bz$ was $>$ 32, which was deemed acceptable, as 30 is an often-used criteria for sufficient sample sizes when data are independent \citep[likely originating in][]{student_probable_1908}.  Most ESS for $\bz$ were much larger than 32.

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 															  EXAMPLE
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------

\section{EXAMPLES}

We illustrate our methods by continuing the motivating examples in Section~\ref{sec:motex}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results of Model Fits
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results of Model Fits}

We first modeled ship-days (Figure~\ref{Fig-raw_data}A) using Equation~\eqref{eq:poireg} with MCMC sampling.  Prior to MCMC sampling, we needed to add zeros to the data (Figure~\ref{Fig-Effort}A).  To add zeros, we used spatial considerations.  Upon close examination, some hexagons had no animals, of any kind, but were completely surrounded by other hexagons with at least one ship-day observation.  It seems highly likely that a ship passed through those hexagons, but no animals were observed, so those remain as missing values.  However, there are other areas of many connected hexagons without any animal observations, and it seems likely that no ships passed through those areas, so they receive zero effort (maroon color in Figure~\ref{Fig-Effort}A). We made the decision to buffer any hexagon with an observed ship-day with missing values, as the ship had to travel to some adjacent hexagon.  However, beyond that, all original missing values were turned to zero, yielding Figure~\ref{Fig-Effort}A.

After burnin and tuning for the MCMC sampler, the mean of the 2000 retained samples (from 2 million MCMC samples) for $\be = \exp(\alpha_{0} + \br)$ is shown in Figure~\ref{Fig-Effort}B.  The mean of the posterior distribution for $\rho$ was greater than 0.99, showing a high amount of autocorrelation.  This resulted in a fairly smooth map of effort that matches what we expect when looking at the raw data (Figure~\ref{Fig-raw_data}A).  We also point out that use of zeros and missing values had the effect of smoothing. For $\be = \exp(\alpha_{0} + \br)$, no values are exactly zero, reflecting the possibility that a ship traveled there but no animals were seen.  Also, hexagons that had missing values, but were surrounded by hexagons with at least two ship-days, had a posterior mean of at least 2 ship-days in the posterior distribution (Figure~\ref{Fig-Effort}B).  In summary, our assessment is that Figure~\ref{Fig-Effort}B is a good reflection of effort.

Next, we modeled northern fur seals (Figure~\ref{Fig-raw_data}B) with MCMC sampling, using Equation~\eqref{eq:poiregoff} with the posterior of $\be = \exp(\alpha_{0} + \br)$ as an offset. By sampling from the posterior distribution of $\be$ during MCMC sampling, we obtain the desired posterior distribution (Equation~\ref{eq:twostage}). Prior to MCMC sampling, we needed to add zeros to the data (Figure~\ref{Fig-raw_data}B), which required different spatial considerations than we used for ship-days. Here, we decided that if the posterior mean of ship-days (effort) was greater than one, but no northern fur seals were seen, then we would assign a zero (maroon color in Figure~\ref{Fig-FurSealFit}A). Otherwise, they were left as missing values.  This reflects the idea that with sufficient effort, animals that are present should be seen.  
 
After burnin and tuning for the MCMC sampler, the mean of the 2000 retained samples (from 2 million MCMC samples) for $\exp(\beta_{0} + \bz)$ is shown in Figure~\ref{Fig-FurSealFit}B.  Note that we set $\log(\be) = \bzero$, so Figure~\ref{Fig-FurSealFit}B represents the expected count per ship-day. The mean of the posterior distribution for $\rho$ was greater than 0.99, showing a high amount of autocorrelation.  The map of northern fur seal distribution shows the known distribution of northern fur seals during the summer months, May - Sept., where northern fur seals are concentrated on the Pribilof Islands in the middle of the Bering Sea during pupping (Figure~\ref{Fig-FurSealFit}B).  Our methods have adjusted for effort. For example, the area of maximum ship-days was located at the pass in the Aleutian Islands which separates the Bering Sea from the Gulf of Alaska (Figure~\ref{Fig-Effort}B).  This resulted in quite a few northern fur seal sightings near this pass as well (Figure~\ref{Fig-FurSealFit}A).  However, after correcting for effort, this pass does not have high fur seal concentrations (Figure~\ref{Fig-FurSealFit}B).

As a second example, we modeled Steller sea lions (Figure~\ref{Fig-raw_data}C), again using Equation~\eqref{eq:poiregoff} with the posterior of $\be = \exp(\alpha_{0} + \br)$ as an offset. Again, we decided that if the posterior mean of ship-days was greater than 1, but no Steller sea lions were seen, that we would assign a zero to a hexagon (maroon color in Figure~\ref{Fig-SeaLionFit}A), otherwise it was left as a missing value. After burnin and tuning for the MCMC sampler, the mean of the 2000 retained samples for $\exp(\beta_{0} + \bz)$ is shown in Figure~\ref{Fig-SeaLionFit}B, representing the expected count per ship-day. Again, the mean of the posterior distribution for $\rho$ was greater than 0.99, showing a high amount of autocorrelation. The map of Steller sea lion distribution shows their known distribution  during the summer months, May - Sept., which is primarily along the coast of the Gulf of Alaska and out into the Aleutian Islands (Figure~\ref{Fig-SeaLionFit}B).

It is also important to understand uncertainty about the maps presented in Figures~\ref{Fig-Effort} through \ref{Fig-SeaLionFit}.  First, we present the posterior standard deviation, hexagon by hexagon, for $\br$ in Equation~\eqref{eq:poireg}.  This is on the log scale in relation to the data.  As we might expect, Figure~\ref{Fig-Uncertainty}A shows standard deviations are lower where we have larger sample sizes (more ship-days) and higher around the edges, which is typical for spatial models.  However, when we look at the posterior standard deviation of $\exp(\alpha_{0} + \br)$ (Figure~\ref{Fig-Uncertainty}B), we see the more typical pattern for count distributions, where the variance is positively related to the mean.  We also show the posterior standard deviation, hexagon by hexagon, for $\bz$ in Equation~\eqref{eq:poiregoff} for northern fur seals (Figure~\ref{Fig-Uncertainty}C) and Steller sea lions (Figure~\ref{Fig-Uncertainty}E).  Again, for the random effects, standard deviations are lowest where values were counted, and highest where data were missing, and around the edges. The posterior standard deviation of the expected counts, $\exp(\beta_{0} + \bz)$, for northern fur seals (Figure~\ref{Fig-Uncertainty}D) and Steller sea lions (Figure~\ref{Fig-Uncertainty}F) show the typical pattern for count distributions, where the variance is positively related to the mean.    
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results from Computing on Posterior Distributions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results from Computing on Posterior Distributions}

One of our primary goals was estimating at-sea abundance, by combining an existing abundance estimate with a standardized species distribution map (Equation~\ref{eq:spexlicitabu}).  We obtained the most current abundance estimate for northern fur seals from the stock assessment report, which is 620,660 seals \citep{muto_alaska_2019}.  No standard error was presented with the estimate, so we hold it fixed. The mode of the posterior density for each grid cell within the DEA provides a spatially-explicit map for northern fur seal (Figure~\ref{Fig-GOAabu}A). For each MCMC iteration, we also summed the abundance estimates for all grid cells within the DEA, providing a total estimate for the DEA. A sample from the posterior distribution of total northern fur seals in the DEA is given as a histogram in Figure~\ref{Fig-GOAabu}B.  Similarly, we obtained the most current abundance estimate for Steller sea lions from the stock assessment report, which is 54,267 sea lions \citep{muto_alaska_2019}; no standard error was given, so we hold it fixed. The mode of the posterior density for each grid cell within the DEA for Steller sea lion is given in Figure~\ref{Fig-GOAabu}C, and a sample from the posterior distribution of total Steller sea lions in the DEA is given as a histogram in Figure~\ref{Fig-GOAabu}D.

Maps, using Equation~\eqref{eq:smoomui} by smoothing over 50 nearest neighbors, are shown for northern fur seals in Figure~\ref{Fig-SmooHotSpots}A, and for Steller sea lions in Figure~\ref{Fig-SmooHotSpots}C.  Standardization of the smoothed values (Equation~\ref{eq:hotspots}), using the highest 10 \%, and using a Bonferonni-adjusted cut-off for an $\alpha$-level of 0.95, yields ``certain hotspots'' for northern fur seals (Figure~\ref{Fig-SmooHotSpots}B) and Steller sea lions (Figure~\ref{Fig-SmooHotSpots}D).  These match our prior experience about areas known to have high abundance for both species.

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 											DISCUSSION AND CONCLUSIONS
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------

\section{DISCUSSION AND CONCLUSIONS}

We used spatial count regression to estimate SDMs for two marine mammals in the Gulf of Alaska and Bering Sea.  We created a hexagonal grid and counted the number of animals per hexagon based on presence-only data collected as shipboard observations without a pre-specified sampling design. To decrease bias, we first estimated a spatial density surface for ship-days, which was our proxy variable for effort.  We created zeros for some hexagons that were far from hexagons with observed animals, and created missing values for those hexagons adjacent to hexagons with observed animals.  We retained an MCMC sample of 2000 spatial surfaces from 2 million iterations from the posterior distribution of ship-days by using spatial negative-binomial regression with random effects that had a multivariate normal distribution with a CAR covariance matrix.

Next, we created SDMs for two species. Here, we created zeros for hexagons that had a mean effort of at least one ship-day and no observed animals, and any remaining hexagons with no observed animals were treated as missing values.  We included the effort surface as an offset in spatial negative-binomial regression with random effects that had a multivariate normal distribution with a CAR covariance matrix, and sampled from the posterior distribution of the effort surface while retaining 2000 samples from 2 million MCMC iterations from the posterior distribution of the SDMs for northern fur seals and Steller sea lions.

From the posterior distributions of the SDMs, we computed two functions of interest.  We normalized the SDMs so that they summed to one, and then applied an overall abundance estimate that we obtained from the literature to derive spatially explicit abundance estimates.  These were then summed in a subset of the study area, the DEA, and the MCMC samples provided a histogram reflecting the posterior distribution of total abundance in the DEA.  The data also identified what we called ``certain hotspots,'' first by smoothing the spatial posterior distributions, dividing each hexagon by the MCMC standard deviation, and then creating thresholds.  Hexagons with values above a threshold were deemed as hotspots with enough evidence to say that we are certain about them.

Our analysis, as indicated by our title, required certain assumptions and decisions, as often occurs when crucial information is missing, which we accumulate and discuss here.

\begin{itemize}

  \item Hierarchical models are highly parametric.  We used negative binomial distributions for count data, and, on the logit scale, a multivariate normal distribution with a CAR covariance matrix for random effects. Both of these were chosen in part for speed and tractability of the ensuing hierarchical model.  These distributions have been combined often in both space \citep[e.g.,][]{wakefield_disease_2007, mohebbi_disease_2014} and time \citep[e.g.,][]{zhu_negative_2011, chen_autoregressive_2016}.
  \item We assumed that the ship-days variable was proportional to effort, following \citet{HimesBoorEtAl2012Stellersealion}, and others have used a similar idea in a different context \citep[e.g.,][]{gomes_species_2018}. If effort is dominated by, say, total counts of the most abundant species, then the modeling of effort and an SDM for that species could be highly confounded, especially for the most abundant species.  The use of ship-days breaks the direct dependence of effort on species counts.  That, along with conditioning and marginalization of the SDM on effort (Equation~\ref{eq:twostage}) decreases, as much as possible, the confounding between effort and the SDM.  In other words, we do not attemp to model the joint distribution of effort and the SDM, but rather treat them sequentially.
  \item We had over 55 years of data but used only spatial locations in the analysis. Hence, for method development, we assumed that the spatial distribution was constant over years.  Our aim is to extend these methods to spatio-temporal models next.
\end{itemize}

We made the following decisions when analyzing the data.

\begin{itemize}

  \item We used hexagons, rather than working with the actual point data.  It would be more natural to use spatial point process models \citep{warton_poisson_2010, renner_equivalence_2013, renner_point_2015}, but the notion of a random surface leads to log-Gaussian Cox-process models \citep{moller_log_1998}, and these are difficult and time-consuming to fit \citep{teng_bayesian_2017}.  A CAR model as a random effect on a grid is a fast approximation for the spatial point process intensity surface \citep{rathbun_space-time_1994}, and still provides an MCMC sample from the posterior distribution \citep{besag_discussion_1994}.  A sensitivity analysis could be performed \citep[][p. 415]{kery_applied_2016} on hexagon size, and ultimately, the coarsest-scale grid that meets objectives should be adopted as the fastest method.  Here, we wanted to show that it is reasonably fast for tens of thousands of samples, so we chose a fine-scale grid.
  \item We did not include any covariates.  This was a major departure from most SDMs, as we reviewed in the Introduction.  We wanted to highlight spatial considerations, especially when adding zeros, and focus on the prospect of creating SDMs without the need for covariates.  Of course, spatial count models for regression can easily allow for covariates, in addition to spatial random effects, to include the best of both methods, and we will focus on this for future research.
  \item We created missing values for the effort (ship-days) data by buffering hexagons with presence-only as missing data, and any hexagons farther away were set as zero.  Obviously, other buffering rules could be applied.  Again, we suggest a sensitivity analysis to examine the effect of buffering on missing values and zeros.
  \item We created zeros for species data by using a threshold based on the posterior distribution for ship-days, setting a hexagon to zero if it had no species counts and a mean posterior value for ship-days greater than 1.  All other hexagons that had no species counts were treated as missing values.  Obviously, thresholds other than 1 ship-day could be tried, and again we recommend sensitivity analysis to explore the effect of various thresholds.
  \item We used truncation limits of $\pm6$ for the normal distribution. This still allowed for a wide range of mean count values (assuming the intercept term is zero), ranging from $\exp(-6) = 0.0025$ to $\exp(6) = 403$ animals per ship-day, which easily encompassed the mean values of our two species (Figures 4B, 5B). Larger values caused problems with MCMC convergence, and smaller values had little effect until truncation limits were less than approximately $\pm4$, when they were too restricted to fit the data well. Truncation can be tailored to the data at hand.
\end{itemize}

A full data analysis should include model diagnostics, and we recommend them here too \citep{conn_guide_2018}.  We have focused on method development, so we omit sensitivity and model-checking.  However, we did perform the most basic model checks -- do the results make sense?  The certain hotspots (Figure~\ref{Fig-SmooHotSpots}A) for northern fur seal are centered on the Pribilof Islands, which is the major breeding center for all northern fur seals in the eastern Bering Sea, so this reflects known distribution for this species.  Interestingly, a second hotspot shows up just west and south of the eastern-most Aleutian Islands, indicating a concentration possibly consisting of animals from Russian rookeries. The main certain hotspots for Steller sea lions (Figure~\ref{Fig-SmooHotSpots}B), from left to right, are the Seguam, Bogoslov, and Shelikof critical habitat foraging areas identified for Stellar sea lions \citep{HimesBoorEtAl2012Stellersealion}.  Two additional hotspots show up farther east, near the outside of the Prince William Sound and the open ocean side of southern southeast Alaska.  Both maps confirm known concentrations, but provide further insight on species distributions.  The histograms for northern fur seal and Steller sea lions (Figure~\ref{Fig-GOAabu}) are both biologically reasonable according to biologists familiar with the species and area.

We have improved on the methods used in \citet{HimesBoorEtAl2012Stellersealion} by using spatial considerations to provide complete maps with smoothing. With some reasonable assumptions, proper models, efficient computing techniques, and a set of analytical decisions, we were able to take presence-only data, along with an overall abundance estimate for each species, and provide spatially explicit abundance estimates, with uncertainties, in a remote part of the Gulf of Alaska with no designed survey effort.  This presents a novel approach to presence-only data to answer important management questions that depend on spatially explicit information.  We stress that surveys designed to provide unbiased population estimates are always preferred but our approach provides important information to natural resource managers when directed scientific survey effort is unavailable.  
 
\section*{Acknowledgments}

The project received financial support from the National Marine Fisheries Service, NOAA. The findings and conclusions in the paper of the author do not necessarily represent the views of the reviewers nor the National Marine Fisheries Service, NOAA. Any use of trade, product or firm names does not imply an endorsement by the U.S. Government.

\section*{Data Availability}

The data are proprietary, containing information from private fishing vessels, with written agreements that these data will not be shared.

\section*{Authors' Contributions}

JVH, MH, and DJ conceived the ideas and designed methodology; RA managed and provided the data; JVH and DJ analysed the data; JVH led the writing of the manuscript. All authors contributed critically to the drafts and gave final approval for publication.

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 											BIBLIOGRAPHY
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------

%\bibliographystyle{consbiol}
\bibliographystyle{asa}
\bibliography{POP.bib}
%\bibliographystyle{/home/jay/Data/shTex/shTex/asa}
%\bibliography{/home/jay/Data/shTex/shTex/StatBibTex.bib}

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 											FIGURES
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------

\newpage
{}

\section*{FIGURES}


\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.8\linewidth]{figures/Fig-study_area-crop}
  \end{center}
  \caption{Study area. A. Data were taken from the Bering Sea and Gulf of Alaska, shown by the light gray shade. The study area was gridded with 11,424 hexagons, but resolution is insufficient to plot them all. The white rectangular inset allows for more detail. The polygon with a dashed black line in the Gulf of Alaska is a Density Extent Area (DEA) used by the U.S. Navy. B.  A close-up of the white rectangular inset, showing hexagonal sample units, each of which was approximately 289 km$^{2}$.  \label{Fig-study_area}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.6\linewidth]{figures/Fig-raw_data-crop}
  \end{center}
  \caption{Raw data used for analyses. A. Ship-days in the study area. Hexagons without any ship days are contained in gray background. B. Northern fur seal counts in each hexagon, where zero counts are part of the gray background. C. Steller sea lion counts in each hexagon.  \label{Fig-raw_data}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.75\linewidth]{figures/Fig-effort-crop}
  \end{center}
  \caption{Effort data and model. A. Ship-days in the study area, where observed counts are the same as Figure~\ref{Fig-raw_data}A, except structural zeros have been added. Hexagons with a gray background were treated as missing data. B. Mode of the posterior distribution for ship-day for each hexagon using a negative binomial regression model with spatial random effects.  \label{Fig-Effort}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.75\linewidth]{figures/Fig-FurSealFit-crop}
  \end{center}
  \caption{Northern fur seal data and model. A. Northern fur seal counts in the study area, where observed counts are the same as Figure~\ref{Fig-raw_data}B, except structural zeros have been added. Hexagons with a gray background were treated as missing data. B. Mode of the posterior distribution for northern fur seals per ship-day for each hexagon using a negative binomial regression model with spatial random effects.   \label{Fig-FurSealFit}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.75\linewidth]{figures/Fig-SeaLionFit-crop}
  \end{center}
  \caption{Steller sea lion data and model. A. Steller sea lion counts in the study area, where observed counts are the same as Figure~\ref{Fig-raw_data}C, except structural zeros have been added. Hexagons with a gray background were treated as missing data. B. Mode of the posterior distribution for Steller sea lions per ship-day for each hexagon using a negative binomial regression model with spatial random effects.  \label{Fig-SeaLionFit}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.8\linewidth]{figures/Fig-Uncertainty-crop}
  \end{center}
  \caption{Model uncertainty. Standard deviation of each hexagon's posterior distribution  for: A) $R_{i}$ in $\br$ from Eq.~\eqref{eq:poireg}, B) $\exp(\alpha_{0} + R_{i})$ from Eq.~\eqref{eq:poireg}, C) $Z_{i}$ in $\bz$ from Eq.~\eqref{eq:poiregoff} for northern fur seals, D) $\exp(\beta_{0} + Z_{i})$ from Eq.~\eqref{eq:poiregoff} for northern fur seals, E) $Z_{i}$ in $\bz$ from Eq.~\eqref{eq:poiregoff} for Steller sea lions, D) $\exp(\beta_{0} + Z_{i})$ from Eq.~\eqref{eq:poiregoff} for Steller sea lions.
  \label{Fig-Uncertainty}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=.9\linewidth]{figures/Fig-GOAabu-crop}
  \end{center}
  \caption{Population estimates per hexagon in the Gulf of Alaska Density Extent Area (DEA). A. Map of mode of the posterior distribution for northern fur seal abundance in DEA. B. Histogram of MCMC sample from posterior distribution for total abundance of northern fur seals in DEA. C. Map of mode of the posterior distribution for Steller sea lions in DEA. D. Histogram of MCMC sample from posterior distribution for total abundance of Steller sea lions in DEA. \label{Fig-GOAabu}}         
\end{figure} 

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=1\linewidth]{figures/Fig-SmooHotSpots-crop}
  \end{center}
  \caption{Smoothing and Hotspots. A. Mode of each hexagon's posterior distribution after smoothing by averaging over 50 nearest neighbors for northern fur seals. B. Certain hotspots created by combining smoothing, thresholds, and uncertainty for northern fur seals. C. Mode of each hexagon's posterior distribution after smoothing by averaging over 50 nearest neighbors for Steller sea lions. D. Certain hotspots created by combining smoothing, thresholds, and uncertainty for Steller sea lions. \label{Fig-SmooHotSpots}}         
\end{figure} 

% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% 							APPENDIX: Details on MCMC Sampling Methods
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------
% ------------------------------------------------------------------------------


\clearpage
\setcounter{equation}{0}
\renewcommand{\theequation}{A.\arabic{equation}}
\setcounter{figure}{0}
%\renewcommand{\thefigure}{A.\arabic{figure}}
%\renewcommand{\thesection}{A.\arabic{section}}
%\setcounter{section}{0}

\section*{APPENDIX: Details on MCMC Sampling Methods} \label{app:MCMCmeth}

\internallinenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Updating Spatial Random Effects Sequentially
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Updating Spatial Random Effects Sequentially}

It would be most natural to set up the Metropolis-Hastings (MH) to sample spatial random effects sequentially. First consider sites with observed counts. The MH acceptance probability for $z_{i}$ at a sampled location $i$ is
$$
\min\left(1,\frac{[y_{i}|\beta_{0},z^{*}_{i},\nu][\by_{-i}|\beta_{0},\bz_{-i},\nu][z^{*}_{i}|\bz_{-i},\sigma,\rho][\bz_{-i}|\sigma,\rho][\beta_{0}] [\sigma] [\rho]q(z_{i}|z_{i}^{*})}
	{[y_{i}|\beta_{0},z_{i},\nu][\by_{-i}|\beta_{0},\bz_{-i},\nu][z_{i}|\bz_{-i},\sigma,\rho][\bz_{-i}|\sigma,\rho][\beta_{0}] [\sigma] [\rho]q(z_{i}^{*}|z_{i})}\right),
$$
where $z_{i}^{*}$ is the proposed new value of $z_{i}$,  $\bz_{-i}$ is the vector $\bz$ minus the $i$th element, and $q(z_{i}|z_{i}^{*})$ is the proposal distribution, which, if symmetric, can be eliminated from the acceptance probability, and recall that $[\by|\beta_{0},\bz,\nu] = [y_{i}|\beta_{0},z_{i},\nu][\by_{-i}|\beta_{0},\bz_{-i},\nu]$ because of conditional independence. Of course, many parts of the above equation cancel, and, assuming a symmetric proposal distribution, yields the acceptance ratio
$$
\min\left(1,\frac{[y_{i}|\beta_{0},z^{*}_{i},\nu][z^{*}_{i}|\bz_{-i},\sigma,\rho]}
	{[y_{i}|\beta_{0},z_{i},\nu][z_{i}|\bz_{-i},\sigma,\rho]}\right).
$$
Due to the special nature of the way we use the CAR model with row-standardization, $[z^{*}_{i}|\bz_{-i},\sigma,\rho] = \textrm{N}(\rho\bar{z}_{i},\sigma^{2}/n_{i})$, where $\bar{z}_{i}$ is the mean of $z$-values that are neighbors of site $i$, and $n_{i}$ is the number of neighbors. This comes directly from the definition of a CAR model. No inversion of matrices are required for one-at-a-time updating of $z$'s at sites with observed counts.  For $z$'s at sites with missing count values, we can do Gibbs sampling straight from the definition.  Again, this is simple, and the only limitation is that looping one-at-a-time might be slow, especially in an interpreted language like R.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Block Updating for Spatial Random Effects
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Block Updating for Spatial Random Effects}

We used block updating, which was faster than sequential sampling. We split the sites into those with observed counts $\bz_{o}$, and those with missing counts $\bz_{m}$. First consider sampling for $\bz_{o}$. The MH acceptance probability is very similar to above,
\begin{equation} \label{eq:MHblock}
\min\left(1,\frac{[\by|\beta_{0},\bz^{*}_{o},\nu][\bz^{*}_{o}|\bz_{m},\sigma,\rho][\bz_{m}|\sigma,\rho][\beta_{0}] [\sigma] [\rho]q(\bz_{o}|\bz_{o}^{*})}
	{[\by|\beta_{0},\bz_{o},\nu][\bz_{o}|\bz_{m},\sigma,\rho][\bz_{m}|\sigma,\rho][\beta_{0}] [\sigma] [\rho]q(\bz_{o}^{*}|\bz_{o})}\right),
\end{equation}
where $\bz^{*}_{o}$ is a proposal for sites with observed counts, $[\by|\beta_{0},\bz_{o},\nu]$ is simply the product of the count distribution model for all sites with counts, and $q(\bz_{o}|\bz_{o}^{*})$ is the proposal distribution. Assuming the proposal distribution is symmetric, simplifying the MH acceptance probability becomes
\begin{equation}\label{eq:MHdrawObs}
\min\left(1,\frac{[\by|\beta_{0},\bz^{*}_{o},\nu][\bz^{*}_{o}|\bz_{m},\sigma,\rho]}
	{[\by|\beta_{0},\bz_{o},\nu][\bz_{o}|\bz_{m},\sigma,\rho]}\right).
\end{equation}
It turns out that can we easily and rapidly draw an MH sample using Equation~\eqref{eq:MHdrawObs}.  

Note that    
\[
\frac{[\by|\beta_{0},\bz^{*}_{o},\nu][\bz^{*}_{o}|\bz_{m},\sigma,\rho]}
	{[\by|\beta_{0},\bz_{o},\nu][\bz_{o}|\bz_{m},\sigma,\rho]} =
  \frac{[\by|\beta_{0},\bz^{*}_{o},\nu][\bz^{*}_{o}, \bz_{m}|\sigma,\rho]}
	{[\by|\beta_{0},\bz_{o},\nu][\bz_{o}, \bz_{m} | \sigma,\rho]}.
\]
So, for a block proposal, $\bz_{o}^{*}$, we can form a MH probability acceptance ratio as
\begin{equation} \label{eq:MHaccProbZo}
\min\left(1,\frac{[\by|\beta_{0},\bz^{*}_{o}]\exp\left(-\frac{1}{2}
(\bz_{m},\bz_{o}^{*})\bSigma\upi \left(
	\begin{array}{c}
		\bz_{m} \\
		\bz_{o}^{*}
	\end{array}
	\right)
\right)}{[\by|\beta_{0},\bz_{o}]\exp\left(-\frac{1}{2}
(\bz_{m},\bz_{o})\bSigma\upi \left(
	\begin{array}{c}
		\bz_{m} \\
		\bz_{o}
	\end{array}
	\right)
\right)}
\right).
\end{equation}
Recall that $\bSigma\upi$ is given directly to us in the CAR model (that is, this is what we are actually modeling), and it is sparse. Equation~\eqref{eq:MHaccProbZo} contains a simple, fast matrix multiplication because we have the full, sparse $\bSigma^{-1}$, and allows for a block update of all $\bz_{o}$.  By the same logic, but without the count distribution, we can do block updates of $\bz_{m}$ as well, 
\begin{equation} \label{eq:MHaccProbZm}
\min\left(1,\frac{\exp\left(-\frac{1}{2}
(\bz_{m}^{*},\bz_{o})\bSigma\upi \left(
	\begin{array}{c}
		\bz_{m}^{*} \\
		\bz_{o}
	\end{array}
	\right)
\right)}{\exp\left(-\frac{1}{2}
(\bz_{m},\bz_{o})\bSigma\upi \left(
	\begin{array}{c}
		\bz_{m} \\
		\bz_{o}
	\end{array}
	\right)
\right)}
\right)
\end{equation}
where $\bz^{*}_{m}$ is the proposal for $\bz_{m}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Problem of Many Zeros, Missing Data, and High Dispersion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Problem of Many Zeros, Missing Data, and High Dispersion}

We ran into a problem because there were many zeros, plus some very high counts, and missing values were unconstrained by observed counts.  The $\bz_{o}$ for sites with observed counts will naturally be sampled so that 
$$
\exp(\beta_{0} + z_{i})
$$
will be near the observed $y_{i} > 0$. However, for $y_{i} = 0$, the ``fit'' will get better and better as $z_{i}$ goes toward negative infinity. This drives up the variance of the latent CAR model while the intercept goes down, and when we predict $z_{i}$ at sites without missing counts, we can obtain large values.  When they are exponentiated, they become even larger, which leads to unrealistically large values on the exponentiated scale.  We considered several solutions, including constraining the CAR variance, but that had limited success because the individual $z_{i}$ still became very large. Instead we treated $\bz$ as a truncated normal distribution to keep any value from becoming too large.  So long as the truncation was not too limiting, it still allowed $z_{i}$ to fit the 0's very well.  The truncation helped to stabilize both the mean and CAR variance parameter as well.  A truncated multivariate normal distribution simply changes the normalizing constant, which  cancels in Equations~(\ref{eq:MHaccProbZo}) and (\ref{eq:MHaccProbZm}), so we only needed to make proposals that stayed within the truncation limits.  We used uniform proposal distributions for each $z_{i}$, where the median of the uniform distribution was centered on $z_{i}$. If $z_{i}$ was near a truncation value, so that the uniform distribution exceeded the truncation limit, then the uniform distribution was simply adjusted so that its endpoint was the truncation limit.  The proposal distribution was symmetric, because both the current values and proposed values have the same probability density from the uniform distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Using Sparse Matrices for Determinants, and Creating a Lookup Table
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Using Sparse Matrices for Determinants, and Creating a Lookup Table}

In the above sections, we showed how to sample $\bz$ quickly using block updates.  Because there are only 2 samplings, one for sites with observed counts, and one for sites with missing counts, it scales linearly with sample size, as only sparse matrix multiplications are involved.  Updates for $\beta_{0}$ and $\sigma$ proceed as usual.  They are very fast because no inverses or determinants are necessary.  However, $\rho$ presents a new challenge because it is involved in the determinant $|\bR_{\rho}|$,
$$
\log[\rho|\ldots] \propto (1/2)\log(|\bR_{\rho}\upi|) - \bz\upp\bR_{\rho}\upi\bz/(2\sigma^{2}),
$$
where $\bR_{\rho}\upi = \bM\upi(\bI - \rho\bW)$ is sparse. Fortunately, determinants for sparse matrices are quite fast. For example, using the sparse matrix representation for a 11,424 $\times$ 11,424 CAR model, we were able to do 81 determinants in several minutes.  These determinants can then be used as a lookup table when doing MCMC sampling.  That is, rather than sampling from $\rho$ continuously, we sample from a grid of values.  The MCMC chain will mix over the discrete values, so very little is lost. The determinant is known from the lookup table, and we may as well store the sparse matrices $\bR_{\rho}\upi=\bM\upi(\bI - \rho\bW)$ for each $\rho$ value as well, so they won't need to be computed during MCMC.

There are some issues with creating the proposals.  Our strategy was to take the nearest 3 indexes on each side of the index of the current $\rho$ value.  For example, we used a grid with 81 $\rho$ values, ordered from lowest to smallest.  They were evenly spaced on the logit scale, so values were compressed near 0 and 1 on the expit scale.  Say the index for the current $\rho$ value is 41. Then, we choose a proposal from indexes 38 to 40, and 42 to 44, with a discrete uniform probability of 1/6.  As long as the current index is not near the ends, the proposal distribution will be symmetric.  However, say the current value has index 81.  Then, the proposal will come from indexes 78:80, with a discrete uniform probability of 1/3 for chosing an index.  Suppose 78 is chosen.  Then a proposal from index 78 would come from indexes 75 to 77 and 79 to 81, with a discrete uniform probability of 1/6 for chosing index 81.  Hence, in general, we used Hastings and included the proposal probabilities $q(\rho|\rho^{*})$ and $q(\rho^{*}|\rho)$,
$$
\min\left(1,\frac{(|\bR_{\rho^{*}}\upi|)^{1/2} \exp[- \by\upp\bR_{\rho^{*}}\upi\by/(2\sigma^{2})]q(\rho|\rho^{*})}
	{(|\bR_{\rho}\upi|)^{1/2} \exp[- \by\upp\bR_{\rho}\upi\by/(2\sigma^{2})]q(\rho^{*}|\rho)}
\right)
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% An Overall Sampling Strategy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{An Overall Sampling Strategy}

When block samping $\bz_{o}$ and $\bz_{m}$, it seems unlikely that, when each vector has 1000's of elements, we can create autocorrelation by independently drawing each of the elements of $\bz_{o}$ and $\bz_{m}$.  This does require literally millions of draws.  So, a good strategy is to draw $\by_{o}$ and $\by_{m}$ many times before advancing to other parameters.  Both of these draws are very fast, simply requiring the proposal, and a sparse matrix multiplication.  We sampled both $\by_{o}$ and $\by_{m}$ twenty times before advancing to other parameters.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%            %%%%%%%    %%%%%%%%  %%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  %%%%%%%%%%%%%%%%%  %  %%%%%%%  %%%%%%%  %%%%  %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  %%%%%%%%%%%%%%%%%  %%  %%%%%%  %%%%%%%  %%%%%%  %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  %%%%%%%%%%%%%%%%%  %%%  %%%%%  %%%%%%%  %%%%%%%   %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%            %%%%%%%  %%%%  %%%%  %%%%%%%  %%%%%%%%  %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  %%%%%%%%%%%%%%%%%  %%%%%  %%%  %%%%%%%  %%%%%%%   %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  %%%%%%%%%%%%%%%%%  %%%%%%  %%  %%%%%%%  %%%%%%  %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  %%%%%%%%%%%%%%%%%  %%%%%%%  %  %%%%%%%  %%%%  %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%            %%%%%%%  %%%%%%%%    %%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{flushleft}
\end{spacing}
\end{document}


